{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "from config import UNetTraining\n",
    "config = UNetTraining.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import rasterio.warp # Reproject raster samples\n",
    "from functools import reduce\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from core.losses import tversky, accuracy, dice_coef, dice_loss, specificity, sensitivity\n",
    "from core.optimizers import adaDelta #, adagrad, adam, nadam\n",
    "\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import warnings # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 256, 64  1792        ['Input[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 256, 256, 64  36928       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['conv2d_1[0][0]']               \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 128, 128, 12  73856       ['max_pooling2d[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 128, 128, 12  147584      ['conv2d_2[0][0]']               \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128, 128, 12  512        ['conv2d_3[0][0]']               \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0          ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 64, 64, 256)  295168      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 64, 64, 256)  590080      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0          ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 512)  1180160     ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 512)  2359808     ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0          ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 1024  4719616     ['max_pooling2d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 1024  9438208     ['conv2d_8[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 32, 32, 1024  0           ['conv2d_9[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 1024  4096       ['up_sampling2d[0][0]']          \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 1536  0           ['batch_normalization_4[0][0]',  \n",
      "                                )                                 'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 512)  7078400     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 512)  2359808     ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 512)  0          ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 64, 512)  2048       ['up_sampling2d_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 768)  0           ['batch_normalization_5[0][0]',  \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 64, 64, 256)  1769728     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 64, 64, 256)  590080      ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 25  0          ['conv2d_13[0][0]']              \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128, 128, 25  1024       ['up_sampling2d_2[0][0]']        \n",
      " rmalization)                   6)                                                                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128, 128, 38  0           ['batch_normalization_6[0][0]',  \n",
      "                                4)                                'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 128, 128, 12  442496      ['concatenate_2[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 128, 128, 12  147584      ['conv2d_14[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 12  0          ['conv2d_15[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 256, 256, 12  512        ['up_sampling2d_3[0][0]']        \n",
      " rmalization)                   8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 256, 256, 19  0           ['batch_normalization_7[0][0]',  \n",
      "                                2)                                'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 256, 256, 64  110656      ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 256, 256, 64  36928       ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 256, 256, 1)  65          ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,390,465\n",
      "Trainable params: 31,384,705\n",
      "Non-trainable params: 5,760\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%run \"2_model.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train_torch = torch.FloatTensor(X_train_reshaped)\\ny_train_torch = torch.FloatTensor(y_train)\\n\\nX_test_torch = torch.FloatTensor(X_test_reshaped)\\ny_test_torch = torch.FloatTensor(y_test)\\n\\nX_val_torch = torch.FloatTensor(X_val)\\ny_val_torch = torch.FloatTensor(y_val)\\n\\ntorch_var = [X_train_torch, y_train_torch, X_test_torch, y_test_torch, X_val_torch, y_val_torch]\\ntype(X_train_torch), X_train_torch.shape'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"X_train_torch = torch.FloatTensor(X_train_reshaped)\n",
    "y_train_torch = torch.FloatTensor(y_train)\n",
    "\n",
    "X_test_torch = torch.FloatTensor(X_test_reshaped)\n",
    "y_test_torch = torch.FloatTensor(y_test)\n",
    "\n",
    "X_val_torch = torch.FloatTensor(X_val)\n",
    "y_val_torch = torch.FloatTensor(y_val)\n",
    "\n",
    "torch_var = [X_train_torch, y_train_torch, X_test_torch, y_test_torch, X_val_torch, y_val_torch]\n",
    "type(X_train_torch), X_train_torch.shape\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getType(variables: list):\n",
    "    for elem in variables:\n",
    "        print(type(elem), elem.dtype)\n",
    "        print(elem.shape, elem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getType([X_train_reshaped, y_train, shape_input])\n",
    "# getType([y_train_tensor])\n",
    "# getType([X_train_torch, y_train_torch, shape_input])\n",
    "# getType([X_train_tensor, y_train_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to gpu\n",
    "# model.device\n",
    "# model.to(device)\n",
    "# model.is_cuda\n",
    "# model_on_gpu = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect GPU\n",
    "# torch.cuda.current_device(), torch.cuda.device_count(), torch.cuda.get_device_name(0)\n",
    "# tf.test.is_gpu_available(), tf.test.gpu_device_name(), tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_ds = tf.data.Dataset.from_tensor_slices((X_train_reshaped, y_train))\\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test_reshaped, y_test))\\n\\nprint(tf.data.experimental.cardinality(train_ds))\\nprint(tf.data.experimental.cardinality(val_ds))\\nprint(tf.data.experimental.cardinality(test_ds))'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_ds = tf.data.Dataset.from_tensor_slices((X_train_reshaped, y_train))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_reshaped, y_test))\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_ds))\n",
    "print(tf.data.experimental.cardinality(val_ds))\n",
    "print(tf.data.experimental.cardinality(test_ds))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for the early stopping of training, LearningRateScheduler and model checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(config.model_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "\n",
    "#reduceonplatea; It can be useful when using adam as optimizer\n",
    "#Reduce learning rate when a metric has stopped improving (after some patience,reduce by a factor of 0.33, new_lr = lr * factor).\n",
    "#cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.33,\n",
    "                                   patience=4, verbose=1, mode='min',\n",
    "                                   min_delta=0.0001, cooldown=4, min_lr=1e-16)\n",
    "\n",
    "#early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=15)\n",
    "\n",
    "log_dir = os.path.join('./logs','UNet_{}_{}_{}_{}_{}'.format(timestr,OPTIMIZER_NAME,LOSS_NAME,chs, config.input_shape[0]))\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard] #reduceLROnPlat is not required with adaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SHUFFLE_BUFFER_SIZE = 100\\n\\ntrain_dataset = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(config.BATCH_SIZE)\\ntest_dataset = test_ds.batch(config.BATCH_SIZE)\\nvalidation_dataset = val_ds.batch(config.BATCH_SIZE)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(config.BATCH_SIZE)\n",
    "test_dataset = test_ds.batch(config.BATCH_SIZE)\n",
    "validation_dataset = val_ds.batch(config.BATCH_SIZE)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds_reshaped = tf.expand_dims(, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_expanded = np.expand_dims(X_train_reshaped, axis=0)\n",
    "# y_train_expanded = np.expand_dims(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.convert_to_tensor(X_train_reshaped)\n",
    "# y = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 3, 256, 256)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(664, 256, 256, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape data to fit model\n",
    "X_train_mini_np = np.array(X_train_mini)\n",
    "X_train_mini_np.shape\n",
    "shape = X_train_mini_np.shape\n",
    "X_train_mini_np = np.reshape(X_train_mini_np, (shape[0], shape[2], shape[3], shape[1]))\n",
    "X_train_mini_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 3, 256, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(208, 256, 256, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape data to fit model\n",
    "X_test_np = np.array(X_test)\n",
    "X_test_np.shape\n",
    "shape = X_test_np.shape\n",
    "X_test_np = np.reshape(X_test_np, (shape[0], shape[2], shape[3], shape[1]))\n",
    "X_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 3, 256, 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(166, 256, 256, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape data to fit model\n",
    "X_val_np = np.array(X_val)\n",
    "X_val_np.shape\n",
    "shape = X_val_np.shape\n",
    "X_val_np = np.reshape(X_val_np, (shape[0], shape[2], shape[3], shape[1]))\n",
    "X_val_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((664, 256, 256, 3), (664, 256, 256), (208, 256, 256, 3))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mini_np.shape, np.array(y_train_mini).shape, X_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2764 - dice_coef: 0.0438 - dice_loss: 0.9562 - specificity: 0.1249 - sensitivity: nan - accuracy: 0.1438\n",
      "Epoch 1: val_loss improved from inf to 0.42768, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 240s 11s/step - loss: 0.2764 - dice_coef: 0.0438 - dice_loss: 0.9562 - specificity: 0.1249 - sensitivity: nan - accuracy: 0.1438 - val_loss: 0.4277 - val_dice_coef: 0.0287 - val_dice_loss: 0.9713 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3920 - dice_coef: 0.0297 - dice_loss: 0.9703 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0149\n",
      "Epoch 2: val_loss improved from 0.42768 to 0.39674, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 233s 11s/step - loss: 0.3920 - dice_coef: 0.0297 - dice_loss: 0.9703 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0149 - val_loss: 0.3967 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2612 - dice_coef: 0.0298 - dice_loss: 0.9702 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0150\n",
      "Epoch 3: val_loss improved from 0.39674 to 0.37860, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.2612 - dice_coef: 0.0298 - dice_loss: 0.9702 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0150 - val_loss: 0.3786 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3934 - dice_coef: 0.0375 - dice_loss: 0.9625 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0188\n",
      "Epoch 4: val_loss improved from 0.37860 to 0.36639, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 232s 11s/step - loss: 0.3934 - dice_coef: 0.0375 - dice_loss: 0.9625 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0188 - val_loss: 0.3664 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2979 - dice_coef: 0.0312 - dice_loss: 0.9688 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0156\n",
      "Epoch 5: val_loss improved from 0.36639 to 0.36088, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 238s 11s/step - loss: 0.2979 - dice_coef: 0.0312 - dice_loss: 0.9688 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0156 - val_loss: 0.3609 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3622 - dice_coef: 0.0348 - dice_loss: 0.9652 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0175\n",
      "Epoch 6: val_loss improved from 0.36088 to 0.35619, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 233s 11s/step - loss: 0.3622 - dice_coef: 0.0348 - dice_loss: 0.9652 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0175 - val_loss: 0.3562 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2937 - dice_coef: 0.0410 - dice_loss: 0.9590 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0206\n",
      "Epoch 7: val_loss improved from 0.35619 to 0.35049, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.2937 - dice_coef: 0.0410 - dice_loss: 0.9590 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0206 - val_loss: 0.3505 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4471 - dice_coef: 0.0285 - dice_loss: 0.9715 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0143\n",
      "Epoch 8: val_loss improved from 0.35049 to 0.34541, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 233s 11s/step - loss: 0.4471 - dice_coef: 0.0285 - dice_loss: 0.9715 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0143 - val_loss: 0.3454 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2458 - dice_coef: 0.0464 - dice_loss: 0.9536 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0233\n",
      "Epoch 9: val_loss improved from 0.34541 to 0.34056, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.2458 - dice_coef: 0.0464 - dice_loss: 0.9536 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0233 - val_loss: 0.3406 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4166 - dice_coef: 0.0235 - dice_loss: 0.9765 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0118\n",
      "Epoch 10: val_loss improved from 0.34056 to 0.33420, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 232s 11s/step - loss: 0.4166 - dice_coef: 0.0235 - dice_loss: 0.9765 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0118 - val_loss: 0.3342 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3013 - dice_coef: 0.0428 - dice_loss: 0.9572 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0215\n",
      "Epoch 11: val_loss improved from 0.33420 to 0.32976, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 238s 11s/step - loss: 0.3013 - dice_coef: 0.0428 - dice_loss: 0.9572 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0215 - val_loss: 0.3298 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2536 - dice_coef: 0.0500 - dice_loss: 0.9500 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0251\n",
      "Epoch 12: val_loss improved from 0.32976 to 0.32254, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 233s 11s/step - loss: 0.2536 - dice_coef: 0.0500 - dice_loss: 0.9500 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0251 - val_loss: 0.3225 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3786 - dice_coef: 0.0269 - dice_loss: 0.9731 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0135\n",
      "Epoch 13: val_loss improved from 0.32254 to 0.31166, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.3786 - dice_coef: 0.0269 - dice_loss: 0.9731 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0135 - val_loss: 0.3117 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2964 - dice_coef: 0.0361 - dice_loss: 0.9639 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0181\n",
      "Epoch 14: val_loss improved from 0.31166 to 0.30117, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 233s 11s/step - loss: 0.2964 - dice_coef: 0.0361 - dice_loss: 0.9639 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0181 - val_loss: 0.3012 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3533 - dice_coef: 0.0285 - dice_loss: 0.9715 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0143\n",
      "Epoch 15: val_loss improved from 0.30117 to 0.29219, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.3533 - dice_coef: 0.0285 - dice_loss: 0.9715 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0143 - val_loss: 0.2922 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3591 - dice_coef: 0.0396 - dice_loss: 0.9604 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0199\n",
      "Epoch 16: val_loss improved from 0.29219 to 0.28609, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 233s 11s/step - loss: 0.3591 - dice_coef: 0.0396 - dice_loss: 0.9604 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0199 - val_loss: 0.2861 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2970 - dice_coef: 0.0333 - dice_loss: 0.9667 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0167\n",
      "Epoch 17: val_loss improved from 0.28609 to 0.28294, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.2970 - dice_coef: 0.0333 - dice_loss: 0.9667 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0167 - val_loss: 0.2829 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2156 - dice_coef: 0.0345 - dice_loss: 0.9655 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0173\n",
      "Epoch 18: val_loss improved from 0.28294 to 0.28175, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 233s 11s/step - loss: 0.2156 - dice_coef: 0.0345 - dice_loss: 0.9655 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0173 - val_loss: 0.2817 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3009 - dice_coef: 0.0362 - dice_loss: 0.9638 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0182\n",
      "Epoch 19: val_loss improved from 0.28175 to 0.28123, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 238s 11s/step - loss: 0.3009 - dice_coef: 0.0362 - dice_loss: 0.9638 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0182 - val_loss: 0.2812 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4087 - dice_coef: 0.0372 - dice_loss: 0.9628 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0187\n",
      "Epoch 20: val_loss did not improve from 0.28123\n",
      "22/22 [==============================] - 230s 10s/step - loss: 0.4087 - dice_coef: 0.0372 - dice_loss: 0.9628 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0187 - val_loss: 0.2818 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1764 - dice_coef: 0.0362 - dice_loss: 0.9638 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0182\n",
      "Epoch 21: val_loss did not improve from 0.28123\n",
      "22/22 [==============================] - 229s 10s/step - loss: 0.1764 - dice_coef: 0.0362 - dice_loss: 0.9638 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0182 - val_loss: 0.2845 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2962 - dice_coef: 0.0315 - dice_loss: 0.9685 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0158\n",
      "Epoch 22: val_loss did not improve from 0.28123\n",
      "22/22 [==============================] - 234s 11s/step - loss: 0.2962 - dice_coef: 0.0315 - dice_loss: 0.9685 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0158 - val_loss: 0.2880 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1560 - dice_coef: 0.0386 - dice_loss: 0.9614 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0194\n",
      "Epoch 23: val_loss did not improve from 0.28123\n",
      "22/22 [==============================] - 230s 10s/step - loss: 0.1560 - dice_coef: 0.0386 - dice_loss: 0.9614 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0194 - val_loss: 0.2892 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3843 - dice_coef: 0.0381 - dice_loss: 0.9619 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0191\n",
      "Epoch 24: val_loss did not improve from 0.28123\n",
      "22/22 [==============================] - 234s 11s/step - loss: 0.3843 - dice_coef: 0.0381 - dice_loss: 0.9619 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0191 - val_loss: 0.2867 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2465 - dice_coef: 0.0348 - dice_loss: 0.9652 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0175\n",
      "Epoch 25: val_loss did not improve from 0.28123\n",
      "22/22 [==============================] - 230s 10s/step - loss: 0.2465 - dice_coef: 0.0348 - dice_loss: 0.9652 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0175 - val_loss: 0.2839 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2530 - dice_coef: 0.0427 - dice_loss: 0.9573 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0214\n",
      "Epoch 26: val_loss did not improve from 0.28123\n",
      "22/22 [==============================] - 234s 11s/step - loss: 0.2530 - dice_coef: 0.0427 - dice_loss: 0.9573 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0214 - val_loss: 0.2814 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3446 - dice_coef: 0.0288 - dice_loss: 0.9712 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0145\n",
      "Epoch 27: val_loss improved from 0.28123 to 0.27951, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 232s 11s/step - loss: 0.3446 - dice_coef: 0.0288 - dice_loss: 0.9712 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0145 - val_loss: 0.2795 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4199 - dice_coef: 0.0321 - dice_loss: 0.9679 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0161\n",
      "Epoch 28: val_loss improved from 0.27951 to 0.27757, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.4199 - dice_coef: 0.0321 - dice_loss: 0.9679 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0161 - val_loss: 0.2776 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1427 - dice_coef: 0.0334 - dice_loss: 0.9666 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0167\n",
      "Epoch 29: val_loss improved from 0.27757 to 0.27631, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 232s 11s/step - loss: 0.1427 - dice_coef: 0.0334 - dice_loss: 0.9666 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0167 - val_loss: 0.2763 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1628 - dice_coef: 0.0474 - dice_loss: 0.9526 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0238\n",
      "Epoch 30: val_loss improved from 0.27631 to 0.26947, saving model to /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 237s 11s/step - loss: 0.1628 - dice_coef: 0.0474 - dice_loss: 0.9526 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0238 - val_loss: 0.2695 - val_dice_coef: 0.0288 - val_dice_loss: 0.9712 - val_specificity: 0.0000e+00 - val_sensitivity: nan - val_accuracy: 0.0145\n",
      "Training completed.\n",
      "CPU times: user 2d 3h 20min 30s, sys: 5h 21min 9s, total: 2d 8h 41min 40s\n",
      "Wall time: 1h 57min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 1:\n",
    "    loss_history = model.fit(x=X_train_mini_np, y=np.array(y_train_mini), #train_ds, # train_dataset,\n",
    "                            epochs=30, # config.NB_EPOCHS, \n",
    "                            validation_data=(X_val_np, np.array(y_val)), # validation_dataset,\n",
    "                            steps_per_epoch=22,#config.MAX_TRAIN_STEPS, # steps_per_epoch * epochs <= # data instances = 94\n",
    "                            # validation_steps=config.VALID_IMG_COUNT,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            callbacks=callbacks_list\n",
    "                            # workers=1,\n",
    "                            # use_multiprocessing=True # the generator is not very thread safe\n",
    "                    )\n",
    "\"\"\"else:\n",
    "    loss_history = [model.fit(x=X, y=y,\n",
    "                            epochs=1, # config.NB_EPOCHS, \n",
    "                            validation_data=val_ds,\n",
    "                            steps_per_epoch=100,#config.MAX_TRAIN_STEPS,\n",
    "                            # validation_steps=config.VALID_IMG_COUNT,\n",
    "                            batch_size=config.BATCH_SIZE,\n",
    "                            callbacks=callbacks_list\n",
    "                            # workers=1,\n",
    "                            # use_multiprocessing=True # the generator is not very thread safe\n",
    "                    )]\n",
    "\"\"\"\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/UNet_20220305-1434_AdaDelta_weightmap_tversky_256.h5'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(config.model_path):\n",
    "    os.makedirs(config.model_path)\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "model_path = os.path.join(config.model_path,'UNet_{}_{}_{}_{}.h5'.format(timestr,OPTIMIZER_NAME,LOSS_NAME,config.input_shape[0]))\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 22s 2s/step - loss: 0.4065 - dice_coef: 0.0318 - dice_loss: 0.9682 - specificity: 0.0000e+00 - sensitivity: nan - accuracy: 0.0160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4064891040325165,\n",
       " 0.03180185332894325,\n",
       " 0.9681981801986694,\n",
       " 0.0,\n",
       " nan,\n",
       " 0.01596304029226303]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test_np, y=np.array(y_test), batch_size=config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = loss_history.history['loss']\n",
    "test_loss = loss_history.history['val_loss']\n",
    "dice_loss = loss_history.history['dice_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
