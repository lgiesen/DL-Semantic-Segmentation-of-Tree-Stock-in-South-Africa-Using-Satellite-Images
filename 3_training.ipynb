{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "config = config.Configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run previous notebooks if necessary\n",
    "if not \"run_prev_notebooks\" in locals(): # use case: this notebook\n",
    "    run_prev_notebooks = False\n",
    "    %run \"1_data_preparation.ipynb\"\n",
    "    %run \"2_model.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "# Define callbacks for the early stopping of training, LearningRateScheduler and model checkpointing\n",
    "checkpoint = ModelCheckpoint(config.model_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "\n",
    "#reduceonplatea; It can be useful when using adam as optimizer\n",
    "#Reduce learning rate when a metric has stopped improving (after some patience,reduce by a factor of 0.33, new_lr = lr * factor).\n",
    "#cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.33,\n",
    "                                   patience=4, verbose=1, mode='min',\n",
    "                                   min_delta=0.0001, cooldown=4, min_lr=1e-16)\n",
    "\n",
    "#early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=15)\n",
    "\n",
    "log_dir = os.path.join(f'./logs/UNet_{timestr}_{OPTIMIZER_NAME}_{LOSS_NAME}_{config.input_shape[0]}')\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard] #reduceLROnPlat is not required with adaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_history = model.fit(\n",
    "        x=np.array(X_train), \n",
    "        y=np.array(y_train),\n",
    "        epochs=config.NB_EPOCHS,\n",
    "        steps_per_epoch=len(X_train) // config.BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        shuffle=True,\n",
    "        validation_steps=len(X_train[0]) // config.BATCH_SIZE,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        callbacks=callbacks_list\n",
    "        # use_multiprocessing=True\n",
    ")\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/saved_data/south_africa_tree_stock/saved_models/UNet_20220314-2341_AdaDelta_weightmap_tversky_256_10.4729_0.0.h5\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(config.model_path):\n",
    "    os.makedirs(config.model_path)\n",
    "\n",
    "acc     = round(100*loss_history.history['accuracy'][-1], 4)\n",
    "val_acc = round(100*loss_history.history['val_accuracy'][-1], 4)\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "model_path = os.path.join(f'{config.model_path}trees_{timestr}_{OPTIMIZER_NAME}_{LOSS_NAME}_{config.input_shape[0]}.h5')\n",
    "print(model_path)\n",
    "model.save(model_path)\n",
    "\n",
    "# The weights without the model architecture can also be saved. Just saving the weights is more efficent.\n",
    "# weight_path=\"./saved_weights/UNet/{}/\".format(timestr)\n",
    "# if not os.path.exists(weight_path):\n",
    "#     os.makedirs(weight_path)\n",
    "# weight_path=weight_path + \"{}_weights.best.hdf5\".format('UNet_model')\n",
    "# print(weight_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
