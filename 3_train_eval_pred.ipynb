{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "from config import UNetTraining\n",
    "# from config import UNetTraining\n",
    "config = UNetTraining.Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "# import imgaug as ia\n",
    "# from imgaug import augmenters as iaa\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import imageio\n",
    "import os\n",
    "import time\n",
    "import rasterio.warp             # Reproject raster samples\n",
    "from functools import reduce\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from core.UNet import UNet\n",
    "from core.losses import tversky, accuracy, dice_coef, dice_loss, specificity, sensitivity\n",
    "from core.optimizers import adaDelta, adagrad, adam, nadam\n",
    "from core.frame_info import FrameInfo\n",
    "# from core.dataset_generator import DataGenerator\n",
    "from core.split_frames import split_dataset\n",
    "from core.visualize import display_images\n",
    "\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # plotting tools\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import warnings                  # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def UNet(input_shape,input_label_channel, layer_count=64, regularizers = regularizers.l2(0.0001), gaussian_noise=0.1, weight_file = None):\n",
    "        \"\"\" Method to declare the UNet model.\n",
    "\n",
    "        Args:\n",
    "            input_shape: tuple(int, int, int, int)\n",
    "                Shape of the input in the format (batch, height, width, channels).\n",
    "            input_label_channel: list([int])\n",
    "                list of index of label channels, used for calculating the number of channels in model output.\n",
    "            layer_count: (int, optional)\n",
    "                Count of kernels in first layer. Number of kernels in other layers grows with a fixed factor.\n",
    "            regularizers: keras.regularizers\n",
    "                regularizers to use in each layer.\n",
    "            weight_file: str\n",
    "                path to the weight file.\n",
    "        \"\"\"\n",
    "\n",
    "        input_img = layers.Input(input_shape[1:], name='Input')\n",
    "        pp_in_layer  = input_img\n",
    "#        pp_in_layer = layers.GaussianNoise(gaussian_noise)(input_img)\n",
    "#        pp_in_layer = layers.BatchNormalization()(pp_in_layer)\n",
    "\n",
    "\n",
    "        c1 = layers.Conv2D(1*layer_count, (3, 3), activation='relu', padding='same')(pp_in_layer)\n",
    "        c1 = layers.Conv2D(1*layer_count, (3, 3), activation='relu', padding='same')(c1)\n",
    "        n1 = layers.BatchNormalization()(c1)\n",
    "        p1 = layers.MaxPooling2D((2, 2))(n1)\n",
    "\n",
    "        c2 = layers.Conv2D(2*layer_count, (3, 3), activation='relu', padding='same')(p1)\n",
    "        c2 = layers.Conv2D(2*layer_count, (3, 3), activation='relu', padding='same')(c2)\n",
    "        n2 = layers.BatchNormalization()(c2)\n",
    "        p2 = layers.MaxPooling2D((2, 2))(n2)\n",
    "\n",
    "        c3 = layers.Conv2D(4*layer_count, (3, 3), activation='relu', padding='same')(p2)\n",
    "        c3 = layers.Conv2D(4*layer_count, (3, 3), activation='relu', padding='same')(c3)\n",
    "        n3 = layers.BatchNormalization()(c3)\n",
    "        p3 = layers.MaxPooling2D((2, 2))(n3)\n",
    "\n",
    "        c4 = layers.Conv2D(8*layer_count, (3, 3), activation='relu', padding='same')(p3)\n",
    "        c4 = layers.Conv2D(8*layer_count, (3, 3), activation='relu', padding='same')(c4)\n",
    "        n4 = layers.BatchNormalization()(c4)\n",
    "        p4 = layers.MaxPooling2D(pool_size=(2, 2))(n4)\n",
    "\n",
    "        c5 = layers.Conv2D(16*layer_count, (3, 3), activation='relu', padding='same')(p4)\n",
    "        c5 = layers.Conv2D(16*layer_count, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "        u6 = layers.UpSampling2D((2, 2))(c5)\n",
    "        n6 = layers.BatchNormalization()(u6)\n",
    "        u6 = layers.concatenate([n6, n4])\n",
    "        c6 = layers.Conv2D(8*layer_count, (3, 3), activation='relu', padding='same')(u6)\n",
    "        c6 = layers.Conv2D(8*layer_count, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "        u7 = layers.UpSampling2D((2, 2))(c6)\n",
    "        n7 = layers.BatchNormalization()(u7)\n",
    "        u7 = layers.concatenate([n7, n3])\n",
    "        c7 = layers.Conv2D(4*layer_count, (3, 3), activation='relu', padding='same')(u7)\n",
    "        c7 = layers.Conv2D(4*layer_count, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "        u8 = layers.UpSampling2D((2, 2))(c7)\n",
    "        n8 = layers.BatchNormalization()(u8)\n",
    "        u8 = layers.concatenate([n8, n2])\n",
    "        c8 = layers.Conv2D(2*layer_count, (3, 3), activation='relu', padding='same')(u8)\n",
    "        c8 = layers.Conv2D(2*layer_count, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "        u9 = layers.UpSampling2D((2, 2))(c8)\n",
    "        n9 = layers.BatchNormalization()(u9)\n",
    "        u9 = layers.concatenate([n9, n1], axis=3)\n",
    "        c9 = layers.Conv2D(1*layer_count, (3, 3), activation='relu', padding='same')(u9)\n",
    "        c9 = layers.Conv2D(1*layer_count, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "        d = layers.Conv2D(len(input_label_channel), (1, 1), activation='sigmoid', kernel_regularizer= regularizers)(c9)\n",
    "\n",
    "        seg_model = models.Model(inputs=[input_img], outputs=[d])\n",
    "        if weight_file:\n",
    "            seg_model.load_weights(weight_file)\n",
    "        seg_model.summary()\n",
    "        return seg_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from Paper Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cutout(satellite, label):\n",
    "    # file =  rasterio.open(complete_filepath)\n",
    "    filepath_satellite, filepath_label = path_satellite + satellite, path_labels + label\n",
    "    # check if filepath exists\n",
    "    if not os.path.isfile(filepath_satellite) or not os.path.isfile(filepath_label):\n",
    "        print(f\"{bcolors.FAIL}1 Failure: File(s) do(es) not exist: {complete_filepath}{bcolors.ENDC}\") \n",
    "        return\n",
    "    \n",
    "    src_label = rasterio.open(filepath_label)\n",
    "    src_satellite = rasterio.open(filepath_satellite)\n",
    "\n",
    "    left, top = src_satellite.bounds[0], src_satellite.bounds[3]\n",
    "    patch_size = config.patch_size[0]\n",
    "    overlap = config.overlap # similar to previous project: 0.12 * 256 = 30.72\n",
    "    xRes, yRes = src_satellite.res # or: src.meta['transform'][0], -src.meta['transform'][4]    \n",
    "    x_cutout_max, y_cutout_max = int(src_satellite.width / patch_size), int(src_satellite.height / patch_size)\n",
    "    # print(\"max cutout:\", x_cutout_max, y_cutout_max)\n",
    "\n",
    "    # tmp testing\n",
    "    x_cutout_max = 5\n",
    "    y_cutout_max = 3\n",
    "    # skip first row, because no data there\n",
    "    # left, top, right, bottom = 29.748021463931796, -26.24839922837354, 29.74923634485858, -26.249614109300325\n",
    "    # left, top = 29.761112169, -26.262076578039085\n",
    "    \n",
    "    for y_cutout in range(y_cutout_max):\n",
    "        for x_cutout in range(x_cutout_max):\n",
    "\n",
    "            right  = left + patch_size * xRes \n",
    "            bottom = top  - patch_size * yRes\n",
    "\n",
    "            # print(\"({}, {}, {}, {})\".format(left, top, right, bottom)) #Cutout Window: \n",
    "            print(\"Size of cutout window in px: {} x {}\".format((right - left) / xRes, (top - bottom) / yRes))\n",
    "            cutout_satellite = src_satellite.read(1, window = from_bounds(left, bottom, right, top, src_satellite.transform))\n",
    "            cutout_label = src_label.read(1, window = from_bounds(left, bottom, right, top, src_label.transform))\n",
    "            \n",
    "            show(cutout_satellite)\n",
    "            show(cutout_label)\n",
    "            \n",
    "            # adjust cutout window for next iteration (with 32 px overlap to cover trees on the edge)\n",
    "            # adjust cutout window on the x-axis => move cutout window to the right - the overlap\n",
    "            left = right - overlap * xRes # use right as a base and move left by the overlap  \n",
    "            right  = left + patch_size * xRes # move patch_size to the right from left # += patch_size * xRes\n",
    "        # reset the x-axis\n",
    "        left = src_satellite.bounds[0]\n",
    "        right  = left + patch_size * xRes\n",
    "        # adjust cutout window on the y-axis => move cutout window to the bottom\n",
    "        top = bottom - overlap * yRes\n",
    "        bottom = top - patch_size * yRes # -= patch_size * yRes\n",
    "\n",
    "    src_satellite.close()\n",
    "    src_label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0: \n",
    "    # Read all images/frames into memory\n",
    "\n",
    "    frames = []\n",
    "    f = FrameInfo(comb_img, annotation) # weight => What does weight do? Used in dataset_generator => maybe use completely different preprocessing altogether3\n",
    "        \n",
    "        # TODO: load satellite and label into one frame?\n",
    "\n",
    "\n",
    "    frames.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0: \n",
    "    # Read all images/frames into memory\n",
    "    frames = []\n",
    "\n",
    "    all_files = os.listdir(config.base_dir)\n",
    "    all_files_ndvi = [fn for fn in all_files if fn.startswith(config.ndvi_fn) and fn.endswith(config.image_type)]\n",
    "    len(all_files_ndvi)\n",
    "    for i, fn in enumerate(all_files_ndvi):\n",
    "        # ndvi_img = rasterio.open(os.path.join(config.base_dir, fn))\n",
    "        # pan_img = rasterio.open(os.path.join(config.base_dir, fn.replace(config.ndvi_fn,config.pan_fn)))\n",
    "        # read_ndvi_img = ndvi_img.read()\n",
    "        # read_pan_img = pan_img.read()\n",
    "        # comb_img = np.concatenate((read_ndvi_img, read_pan_img), axis=0)\n",
    "        # comb_img = np.transpose(comb_img, axes=(1,2,0)) #Channel at the end\n",
    "        # annotation_im = Image.open(os.path.join(config.base_dir, fn.replace(config.ndvi_fn,config.annotation_fn)))\n",
    "        # annotation = np.array(annotation_im)\n",
    "        # weight_im = Image.open(os.path.join(config.base_dir, fn.replace(config.ndvi_fn,config.weight_fn)))\n",
    "        # weight = np.array(weight_im)\n",
    "\n",
    "\n",
    "        f = FrameInfo(comb_img, annotation) # weight => What does weight do? Used in dataset_generator => maybe use completely different preprocessing altogether3\n",
    "        \n",
    "        # TODO: load satellite and label into one frame?\n",
    "\n",
    "\n",
    "        frames.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frames, validation_frames, testing_frames  = split_dataset(frames, config.frames_json, config.patch_dir)\n",
    "# training_frames = validation_frames = testing_frames  = list(range(len(frames)))\n",
    "\n",
    "annotation_channels = config.input_label_channel + config.input_weight_channel\n",
    "# DataGenerator performs Data Augmentation: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "train_generator = DataGenerator(config.input_image_channel, config.patch_size, training_frames, frames, annotation_channels, augmenter = 'iaa').random_generator(config.BATCH_SIZE, normalize = config.normalize)\n",
    "val_generator = DataGenerator(config.input_image_channel, config.patch_size, validation_frames, frames, annotation_channels, augmenter= None).random_generator(config.BATCH_SIZE, normalize = config.normalize)\n",
    "test_generator = DataGenerator(config.input_image_channel, config.patch_size, testing_frames, frames, annotation_channels, augmenter= None).random_generator(config.BATCH_SIZE, normalize = config.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    train_images, real_label = next(train_generator)\n",
    "    ann = real_label[:,:,:,0]\n",
    "    wei = real_label[:,:,:,1]\n",
    "    #overlay of annotation with boundary to check the accuracy\n",
    "    #5 images in each row are: pan, ndvi, annotation, weight(boundary), overlay of annotation with weight\n",
    "    overlay = ann + wei\n",
    "    overlay = overlay[:,:,:,np.newaxis]\n",
    "    display_images(np.concatenate((train_images,real_label, overlay), axis = -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = adaDelta\n",
    "LOSS = tversky \n",
    "\n",
    "#Only for the name of the model in the very end\n",
    "OPTIMIZER_NAME = 'AdaDelta'\n",
    "LOSS_NAME = 'weightmap_tversky'\n",
    "\n",
    "# Declare the path to the final model\n",
    "# If you want to retrain an exising model then change the cell where model is declared. \n",
    "# This path is for storing a model after training.\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "chf = config.input_image_channel + config.input_label_channel\n",
    "chs = reduce(lambda a,b: a+str(b), chf, '')\n",
    "\n",
    "\n",
    "if not os.path.exists(config.model_path):\n",
    "    os.makedirs(config.model_path)\n",
    "model_path = os.path.join(config.model_path,'trees_{}_{}_{}_{}_{}.h5'.format(timestr,OPTIMIZER_NAME,LOSS_NAME,chs,config.input_shape[0]))\n",
    "\n",
    "# The weights without the model architecture can also be saved. Just saving the weights is more efficent.\n",
    "\n",
    "# weight_path=\"./saved_weights/UNet/{}/\".format(timestr)\n",
    "# if not os.path.exists(weight_path):\n",
    "#     os.makedirs(weight_path)\n",
    "# weight_path=weight_path + \"{}_weights.best.hdf5\".format('UNet_model')\n",
    "# print(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 512, 512, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 512, 512, 64) 2368        Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 512, 512, 64) 256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 256, 256, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 128 73856       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 128 147584      conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256, 256, 128 512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 128 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, 128, 256 295168      max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, 128, 256 590080      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 128, 256 1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 64, 64, 512)  1180160     max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 512)  2048        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 1024) 4719616     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 1024) 9438208     conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 64, 64, 1024) 0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 1024) 4096        up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 1536) 0           batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 512)  7078400     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 512 2048        up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 128, 768 0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 128, 128, 256 1769728     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 256 590080      conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256, 256, 256 1024        up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256, 256, 384 0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 256, 256, 128 442496      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 128 147584      conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 512, 512, 128 512         up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 512, 512, 192 0           batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 512, 512, 64) 110656      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 512, 512, 1)  65          conv2d_36[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,391,041\n",
      "Trainable params: 31,385,281\n",
      "Non-trainable params: 5,760\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model and compile it\n",
    "model = UNet([config.BATCH_SIZE, *config.input_shape],config.input_label_channel)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[dice_coef, dice_loss, specificity, sensitivity, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet([config.BATCH_SIZE, *config.input_shape],config.input_label_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for the early stopping of training, LearningRateScheduler and model checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = False)\n",
    "\n",
    "#reduceonplatea; It can be useful when using adam as optimizer\n",
    "#Reduce learning rate when a metric has stopped improving (after some patience,reduce by a factor of 0.33, new_lr = lr * factor).\n",
    "#cooldown: number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.33,\n",
    "                                   patience=4, verbose=1, mode='min',\n",
    "                                   min_delta=0.0001, cooldown=4, min_lr=1e-16)\n",
    "\n",
    "#early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=15)\n",
    "\n",
    "log_dir = os.path.join('./logs','UNet_{}_{}_{}_{}_{}'.format(timestr,OPTIMIZER_NAME,LOSS_NAME,chs, config.input_shape[0]))\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "callbacks_list = [checkpoint, tensorboard] #reduceLROnPlat is not required with adaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ed2b45c86ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loss_history = [model.fit(train_generator, \n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_TRAIN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNB_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALID_IMG_COUNT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "loss_history = [model.fit(train_generator, \n",
    "                         steps_per_epoch=config.MAX_TRAIN_STEPS, \n",
    "                         epochs=config.NB_EPOCHS, \n",
    "                         validation_data=val_generator,\n",
    "                         validation_steps=config.VALID_IMG_COUNT,\n",
    "                         callbacks=callbacks_list,\n",
    "                         workers=1,\n",
    "#                         use_multiprocessing=True # the generator is not very thread safe\n",
    "                        )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. YT Video: Preview of Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load previously saved model\n",
    "from keras.models import load_model\n",
    "# model = load_model(\"/content/drive/MyDrive/Colab Notebooks/saved_models/tutorial118_mitochondria_25epochs.hdf5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IOU\n",
    "y_pred=model.predict(X_test)\n",
    "# if model is more than 50% sure it is a cell, it returns True, else False\n",
    "y_pred_thresholded = y_pred > 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import MeanIoU\n",
    "n_classes = 2\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_pred_thresholded, y_test)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "test_img_number = random.randint(0, len(X_test)-1)\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=y_test[test_img_number]\n",
    "test_img_input=np.expand_dims(test_img, 0)\n",
    "print(test_img_input.shape)\n",
    "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.5).astype(np.uint8)\n",
    "print(prediction.shape)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(prediction, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
